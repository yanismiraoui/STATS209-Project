{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects:  1009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['timeseires', 'label', 'corr', 'pcorr', 'site', 'id'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(f\"./data/abide.npy\",allow_pickle=True).item()\n",
    "print(\"Number of subjects: \", len(data['label']))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectome (X) shape: (1009, 116, 116)\n"
     ]
    }
   ],
   "source": [
    "conn = data[\"corr\"]\n",
    "print(f\"Connectome (X) shape: {conn.shape}\") # n_ROIs, n_ROIs, n_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD scores (y) shape: (1009,)\n"
     ]
    }
   ],
   "source": [
    "scores = data[\"label\"]\n",
    "print(f\"ASD scores (y) shape: {scores.shape}\") # n_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject IDs shape: 1009\n"
     ]
    }
   ],
   "source": [
    "subject_ids = [int(i) for i in data[\"id\"]]\n",
    "print(f\"Subject IDs shape: {len(subject_ids)}\") # n_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(13456, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, latent_dim)  # Compressed representation\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 13456),\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (807, 116, 116)\n",
      "X_test shape: (202, 116, 116)\n",
      "y_train shape: (807,)\n",
      "y_test shape: (202,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(conn, scores, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (800, 116, 116)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "# change the length of X_train to match batch_size\n",
    "X_train = X_train[:len(X_train) - (len(X_train) % batch_size)]\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "train_loader = DataLoader(X_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0389, Test loss: 0.0368\n",
      "Epoch [2/10], Loss: 0.0258, Test loss: 0.0360\n",
      "Epoch [3/10], Loss: 0.0921, Test loss: 0.0354\n",
      "Epoch [4/10], Loss: 0.0174, Test loss: 0.0348\n",
      "Epoch [5/10], Loss: 0.0335, Test loss: 0.0339\n",
      "Epoch [6/10], Loss: 0.0239, Test loss: 0.0339\n",
      "Epoch [7/10], Loss: 0.0311, Test loss: 0.0342\n",
      "Epoch [8/10], Loss: 0.0300, Test loss: 0.0338\n",
      "Epoch [9/10], Loss: 0.0425, Test loss: 0.0337\n",
      "Epoch [10/10], Loss: 0.0293, Test loss: 0.0337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for d in train_loader:\n",
    "        matrix = d.float()\n",
    "        # reshape\n",
    "        matrix = matrix.view(matrix.size(0), -1)\n",
    "        output = model(matrix)\n",
    "        loss = criterion(output, matrix)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate on test set\n",
    "    test_matrix = torch.tensor(X_test).float()\n",
    "    test_matrix = test_matrix.view(test_matrix.size(0), -1)\n",
    "    test_output = model(test_matrix)\n",
    "    test_loss = criterion(test_output, test_matrix)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test loss: {test_loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape: (1009, 116, 116)\n",
      "Adapted X shape: (1009, 116, 116)\n",
      "Epoch [1/10], Loss: 0.0569\n",
      "Epoch [2/10], Loss: 0.0383\n",
      "Epoch [3/10], Loss: 0.0311\n",
      "Epoch [4/10], Loss: 0.0594\n",
      "Epoch [5/10], Loss: 0.0515\n",
      "Epoch [6/10], Loss: 0.0371\n",
      "Epoch [7/10], Loss: 0.0340\n",
      "Epoch [8/10], Loss: 0.0293\n",
      "Epoch [9/10], Loss: 0.0392\n",
      "Epoch [10/10], Loss: 0.0422\n"
     ]
    }
   ],
   "source": [
    "# Train on all data as it is an unsupervised task\n",
    "\n",
    "# Initiliaze model\n",
    "model = Autoencoder().float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Determine batch size and adapt dataset\n",
    "batch_size = 1\n",
    "# change the length of X_train to match batch_size\n",
    "conn = conn[:len(conn) - (len(conn) % batch_size)]\n",
    "subject_ids = subject_ids[:len(subject_ids) - (len(subject_ids) % batch_size)]\n",
    "print(f\"Initial X shape: {conn.shape}\")\n",
    "print(f\"Adapted X shape: {conn.shape}\")\n",
    "X_loader = DataLoader(conn, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for d in X_loader:\n",
    "        matrix = d.float()\n",
    "        # reshape\n",
    "        matrix = matrix.view(matrix.size(0), -1)\n",
    "        output = model(matrix)\n",
    "        loss = criterion(output, matrix)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0327\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for all data (train and test combined)\n",
    "all_data = torch.tensor(conn).float()\n",
    "all_data = all_data.view(all_data.size(0), -1)\n",
    "all_output = model(all_data)\n",
    "all_loss = criterion(all_output, all_data)\n",
    "print(f'Loss: {all_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed representation shape: (1009, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.259695  ,  2.2921166 ,  1.9402783 ],\n",
       "       [-0.6182181 ,  2.4837327 ,  2.222906  ],\n",
       "       [-0.5641706 ,  0.11749941,  0.26383722],\n",
       "       ...,\n",
       "       [-1.2959974 ,  2.0995135 ,  1.8607417 ],\n",
       "       [-2.4122214 ,  2.0182457 ,  1.697311  ],\n",
       "       [-1.9440824 ,  2.0219364 ,  1.7408917 ]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the compressed representation\n",
    "compressed = model.encoder(all_data)\n",
    "compressed = compressed.detach().numpy()\n",
    "print(f\"Compressed representation shape: {compressed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SUB_ID</th>\n",
       "      <th>X</th>\n",
       "      <th>subject</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DSM_IV_TR</th>\n",
       "      <th>AGE_AT_SCAN</th>\n",
       "      <th>SEX</th>\n",
       "      <th>...</th>\n",
       "      <th>qc_notes_rater_1</th>\n",
       "      <th>qc_anat_rater_2</th>\n",
       "      <th>qc_anat_notes_rater_2</th>\n",
       "      <th>qc_func_rater_2</th>\n",
       "      <th>qc_func_notes_rater_2</th>\n",
       "      <th>qc_anat_rater_3</th>\n",
       "      <th>qc_anat_notes_rater_3</th>\n",
       "      <th>qc_func_rater_3</th>\n",
       "      <th>qc_func_notes_rater_3</th>\n",
       "      <th>SUB_IN_SMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>PITT</td>\n",
       "      <td>no_filename</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.77</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fail</td>\n",
       "      <td>ic-parietal-cerebellum</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fail</td>\n",
       "      <td>ERROR #24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.45</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.73</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-parietal-cerebellum</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-parietal slight</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SUB_ID  X  subject SITE_ID       FILE_ID  DX_GROUP  DSM_IV_TR  \\\n",
       "0           1   50002  1    50002    PITT   no_filename         1          1   \n",
       "1           2   50003  2    50003    PITT  Pitt_0050003         1          1   \n",
       "2           3   50004  3    50004    PITT  Pitt_0050004         1          1   \n",
       "3           4   50005  4    50005    PITT  Pitt_0050005         1          1   \n",
       "4           5   50006  5    50006    PITT  Pitt_0050006         1          1   \n",
       "\n",
       "   AGE_AT_SCAN  SEX  ... qc_notes_rater_1  qc_anat_rater_2  \\\n",
       "0        16.77    1  ...              NaN               OK   \n",
       "1        24.45    1  ...              NaN               OK   \n",
       "2        19.09    1  ...              NaN               OK   \n",
       "3        13.73    2  ...              NaN               OK   \n",
       "4        13.37    1  ...              NaN               OK   \n",
       "\n",
       "   qc_anat_notes_rater_2  qc_func_rater_2   qc_func_notes_rater_2  \\\n",
       "0                    NaN             fail  ic-parietal-cerebellum   \n",
       "1                    NaN               OK                     NaN   \n",
       "2                    NaN               OK                     NaN   \n",
       "3                    NaN            maybe  ic-parietal-cerebellum   \n",
       "4                    NaN            maybe      ic-parietal slight   \n",
       "\n",
       "  qc_anat_rater_3 qc_anat_notes_rater_3 qc_func_rater_3  \\\n",
       "0              OK                   NaN            fail   \n",
       "1              OK                   NaN              OK   \n",
       "2              OK                   NaN              OK   \n",
       "3              OK                   NaN              OK   \n",
       "4              OK                   NaN              OK   \n",
       "\n",
       "   qc_func_notes_rater_3  SUB_IN_SMP  \n",
       "0              ERROR #24           1  \n",
       "1                    NaN           1  \n",
       "2                    NaN           1  \n",
       "3                    NaN           0  \n",
       "4                    NaN           1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load original csv file\n",
    "PATH_CSV = \"./data/ABIDE_tab.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH_CSV, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 1009\n",
      "Number of compressed values: 1009\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to hold sub_id and corresponding compressed values\n",
    "print(f\"Number of subjects: {len(subject_ids)}\")\n",
    "print(f\"Number of compressed values: {len(compressed)}\")\n",
    "compressed_dict = {sub_id: compressed[i] for i, sub_id in enumerate(subject_ids)}\n",
    "\n",
    "# Function to return the compressed value for a given sub_id\n",
    "def get_compressed_value(row, subject_ids, compressed):\n",
    "    if row['SUB_ID'] in subject_ids:\n",
    "        index = subject_ids.index(row['SUB_ID'])\n",
    "        return compressed[index]\n",
    "    return np.nan  # or return the default value you want in case of no match\n",
    "\n",
    "# Apply this function to each row in the DataFrame\n",
    "df['compressed'] = df.apply(get_compressed_value, axis=1, args=(subject_ids, compressed))\n",
    "\n",
    "# Save df to csv\n",
    "df.to_csv(\"./data/ABIDE_tab_compressed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 1112\n",
      "Number of compressed values: 1009\n",
      "Number of subjects with no MRI: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of subjects: {len(df)}\")\n",
    "print(f\"Number of compressed values: {len(compressed)}\")\n",
    "print(f\"Number of subjects with no MRI: {df['compressed'].isna().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the same pipeline for different size of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent size: 2\n",
      "Epoch [1/10], Loss: 0.0505\n",
      "Epoch [2/10], Loss: 0.0501\n",
      "Epoch [3/10], Loss: 0.0598\n",
      "Epoch [4/10], Loss: 0.0273\n",
      "Epoch [5/10], Loss: 0.0504\n",
      "Epoch [6/10], Loss: 0.0227\n",
      "Epoch [7/10], Loss: 0.0298\n",
      "Epoch [8/10], Loss: 0.0281\n",
      "Epoch [9/10], Loss: 0.0469\n",
      "Epoch [10/10], Loss: 0.0395\n",
      "Loss: 0.0326\n",
      "Compressed representation shape: (1009, 2)\n",
      "Latent size: 3\n",
      "Epoch [1/10], Loss: 0.0436\n",
      "Epoch [2/10], Loss: 0.0407\n",
      "Epoch [3/10], Loss: 0.0305\n",
      "Epoch [4/10], Loss: 0.0244\n",
      "Epoch [5/10], Loss: 0.0262\n",
      "Epoch [6/10], Loss: 0.0359\n",
      "Epoch [7/10], Loss: 0.0272\n",
      "Epoch [8/10], Loss: 0.0309\n",
      "Epoch [9/10], Loss: 0.0493\n",
      "Epoch [10/10], Loss: 0.0385\n",
      "Loss: 0.0328\n",
      "Compressed representation shape: (1009, 3)\n",
      "Latent size: 4\n",
      "Epoch [1/10], Loss: 0.0253\n",
      "Epoch [2/10], Loss: 0.0272\n",
      "Epoch [3/10], Loss: 0.0436\n",
      "Epoch [4/10], Loss: 0.0306\n",
      "Epoch [5/10], Loss: 0.0298\n",
      "Epoch [6/10], Loss: 0.0498\n",
      "Epoch [7/10], Loss: 0.0327\n",
      "Epoch [8/10], Loss: 0.0322\n",
      "Epoch [9/10], Loss: 0.0324\n",
      "Epoch [10/10], Loss: 0.0299\n",
      "Loss: 0.0326\n",
      "Compressed representation shape: (1009, 4)\n",
      "Latent size: 6\n",
      "Epoch [1/10], Loss: 0.0270\n",
      "Epoch [2/10], Loss: 0.0249\n",
      "Epoch [3/10], Loss: 0.0473\n",
      "Epoch [4/10], Loss: 0.0387\n",
      "Epoch [5/10], Loss: 0.0385\n",
      "Epoch [6/10], Loss: 0.0455\n",
      "Epoch [7/10], Loss: 0.0283\n",
      "Epoch [8/10], Loss: 0.0368\n",
      "Epoch [9/10], Loss: 0.0434\n",
      "Epoch [10/10], Loss: 0.0308\n",
      "Loss: 0.0322\n",
      "Compressed representation shape: (1009, 6)\n",
      "Latent size: 8\n",
      "Epoch [1/10], Loss: 0.0548\n",
      "Epoch [2/10], Loss: 0.0817\n",
      "Epoch [3/10], Loss: 0.0315\n",
      "Epoch [4/10], Loss: 0.0431\n",
      "Epoch [5/10], Loss: 0.0231\n",
      "Epoch [6/10], Loss: 0.0343\n",
      "Epoch [7/10], Loss: 0.0248\n",
      "Epoch [8/10], Loss: 0.0272\n",
      "Epoch [9/10], Loss: 0.0363\n",
      "Epoch [10/10], Loss: 0.0327\n",
      "Loss: 0.0323\n",
      "Compressed representation shape: (1009, 8)\n",
      "Latent size: 10\n",
      "Epoch [1/10], Loss: 0.0487\n",
      "Epoch [2/10], Loss: 0.0319\n",
      "Epoch [3/10], Loss: 0.0435\n",
      "Epoch [4/10], Loss: 0.0283\n",
      "Epoch [5/10], Loss: 0.0344\n",
      "Epoch [6/10], Loss: 0.0277\n",
      "Epoch [7/10], Loss: 0.0323\n",
      "Epoch [8/10], Loss: 0.0549\n",
      "Epoch [9/10], Loss: 0.0390\n",
      "Epoch [10/10], Loss: 0.0286\n",
      "Loss: 0.0327\n",
      "Compressed representation shape: (1009, 10)\n"
     ]
    }
   ],
   "source": [
    "latent_sizes = [2, 3, 4, 6, 8, 10]\n",
    "for latent_size in latent_sizes:\n",
    "    print(f\"Latent size: {latent_size}\")\n",
    "    # Initiliaze model\n",
    "    model = Autoencoder(latent_dim=latent_size).float()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    X_loader = DataLoader(conn, batch_size=1, shuffle=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for d in X_loader:\n",
    "            matrix = d.float()\n",
    "            matrix = matrix.view(matrix.size(0), -1)\n",
    "            output = model(matrix)\n",
    "            loss = criterion(output, matrix)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Get predictions for all data (train and test combined)\n",
    "    all_data = torch.tensor(conn).float()\n",
    "    all_data = all_data.view(all_data.size(0), -1)\n",
    "    all_output = model(all_data)\n",
    "    all_loss = criterion(all_output, all_data)\n",
    "    print(f'Loss: {all_loss.item():.4f}')\n",
    "\n",
    "\n",
    "    # Get the compressed representation\n",
    "    compressed = model.encoder(all_data)\n",
    "    compressed = compressed.detach().numpy()\n",
    "    print(f\"Compressed representation shape: {compressed.shape}\")\n",
    "\n",
    "    # Create a dictionary to hold sub_id and corresponding compressed values\n",
    "    compressed_dict = {sub_id: compressed[i] for i, sub_id in enumerate(subject_ids)}\n",
    "\n",
    "    # Apply this function to each row in the DataFrame\n",
    "    df[f\"compressed_{latent_size}\"] = df.apply(get_compressed_value, axis=1, args=(subject_ids, compressed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SUB_ID</th>\n",
       "      <th>X</th>\n",
       "      <th>subject</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DSM_IV_TR</th>\n",
       "      <th>AGE_AT_SCAN</th>\n",
       "      <th>SEX</th>\n",
       "      <th>...</th>\n",
       "      <th>qc_anat_notes_rater_3</th>\n",
       "      <th>qc_func_rater_3</th>\n",
       "      <th>qc_func_notes_rater_3</th>\n",
       "      <th>SUB_IN_SMP</th>\n",
       "      <th>compressed_2</th>\n",
       "      <th>compressed_3</th>\n",
       "      <th>compressed_4</th>\n",
       "      <th>compressed_6</th>\n",
       "      <th>compressed_8</th>\n",
       "      <th>compressed_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>PITT</td>\n",
       "      <td>no_filename</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.77</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fail</td>\n",
       "      <td>ERROR #24</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.45</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.44941542, -2.9575777]</td>\n",
       "      <td>[1.1815448, -4.634217, 3.114023]</td>\n",
       "      <td>[3.8482342, -0.3381914, 1.3678892, -2.8299541]</td>\n",
       "      <td>[0.47832513, -0.3344283, -2.775017, -0.6487376...</td>\n",
       "      <td>[-3.2448077, 0.20148331, 0.729448, 1.8987474, ...</td>\n",
       "      <td>[-2.5745575, -0.6420369, -1.4933676, 1.4467326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4390367, -1.8658609]</td>\n",
       "      <td>[0.39401212, -2.9582384, 2.079665]</td>\n",
       "      <td>[2.6737583, 0.08604087, 0.8983698, -1.8406199]</td>\n",
       "      <td>[-0.39788154, 0.9797256, -0.345452, -0.6684227...</td>\n",
       "      <td>[-1.6996136, 0.58251894, 1.1431954, 1.3389084,...</td>\n",
       "      <td>[-1.0550424, -0.4334481, -1.2319748, 0.5235287...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.73</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5495021, -1.2372564]</td>\n",
       "      <td>[-2.4866755, -2.2959528, 1.7506592]</td>\n",
       "      <td>[3.2064295, 1.6908103, 1.285104, -1.3762128]</td>\n",
       "      <td>[-0.5110897, -1.0571824, -2.980431, -3.9370008...</td>\n",
       "      <td>[-2.2855935, 0.7342312, 3.1883984, 2.0930343, ...</td>\n",
       "      <td>[2.9089928, -1.321404, -3.329828, -1.644749, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.50030065, -0.7813915]</td>\n",
       "      <td>[-0.46178344, -1.350864, 1.0396112]</td>\n",
       "      <td>[1.7077237, 0.42651868, 0.51136065, -1.0311688]</td>\n",
       "      <td>[-0.29313406, 0.8351699, -0.017895207, -0.4916...</td>\n",
       "      <td>[-0.57433647, 0.7514796, 1.2304022, 0.8044821,...</td>\n",
       "      <td>[0.34323144, -0.16988352, -0.8524246, -0.33802...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SUB_ID  X  subject SITE_ID       FILE_ID  DX_GROUP  DSM_IV_TR  \\\n",
       "0           1   50002  1    50002    PITT   no_filename         1          1   \n",
       "1           2   50003  2    50003    PITT  Pitt_0050003         1          1   \n",
       "2           3   50004  3    50004    PITT  Pitt_0050004         1          1   \n",
       "3           4   50005  4    50005    PITT  Pitt_0050005         1          1   \n",
       "4           5   50006  5    50006    PITT  Pitt_0050006         1          1   \n",
       "\n",
       "   AGE_AT_SCAN  SEX  ... qc_anat_notes_rater_3  qc_func_rater_3  \\\n",
       "0        16.77    1  ...                   NaN             fail   \n",
       "1        24.45    1  ...                   NaN               OK   \n",
       "2        19.09    1  ...                   NaN               OK   \n",
       "3        13.73    2  ...                   NaN               OK   \n",
       "4        13.37    1  ...                   NaN               OK   \n",
       "\n",
       "   qc_func_notes_rater_3  SUB_IN_SMP              compressed_2  \\\n",
       "0              ERROR #24           1                       NaN   \n",
       "1                    NaN           1  [0.44941542, -2.9575777]   \n",
       "2                    NaN           1   [0.4390367, -1.8658609]   \n",
       "3                    NaN           0   [1.5495021, -1.2372564]   \n",
       "4                    NaN           1  [0.50030065, -0.7813915]   \n",
       "\n",
       "                          compressed_3  \\\n",
       "0                                  NaN   \n",
       "1     [1.1815448, -4.634217, 3.114023]   \n",
       "2   [0.39401212, -2.9582384, 2.079665]   \n",
       "3  [-2.4866755, -2.2959528, 1.7506592]   \n",
       "4  [-0.46178344, -1.350864, 1.0396112]   \n",
       "\n",
       "                                      compressed_4  \\\n",
       "0                                              NaN   \n",
       "1   [3.8482342, -0.3381914, 1.3678892, -2.8299541]   \n",
       "2   [2.6737583, 0.08604087, 0.8983698, -1.8406199]   \n",
       "3     [3.2064295, 1.6908103, 1.285104, -1.3762128]   \n",
       "4  [1.7077237, 0.42651868, 0.51136065, -1.0311688]   \n",
       "\n",
       "                                        compressed_6  \\\n",
       "0                                                NaN   \n",
       "1  [0.47832513, -0.3344283, -2.775017, -0.6487376...   \n",
       "2  [-0.39788154, 0.9797256, -0.345452, -0.6684227...   \n",
       "3  [-0.5110897, -1.0571824, -2.980431, -3.9370008...   \n",
       "4  [-0.29313406, 0.8351699, -0.017895207, -0.4916...   \n",
       "\n",
       "                                        compressed_8  \\\n",
       "0                                                NaN   \n",
       "1  [-3.2448077, 0.20148331, 0.729448, 1.8987474, ...   \n",
       "2  [-1.6996136, 0.58251894, 1.1431954, 1.3389084,...   \n",
       "3  [-2.2855935, 0.7342312, 3.1883984, 2.0930343, ...   \n",
       "4  [-0.57433647, 0.7514796, 1.2304022, 0.8044821,...   \n",
       "\n",
       "                                       compressed_10  \n",
       "0                                                NaN  \n",
       "1  [-2.5745575, -0.6420369, -1.4933676, 1.4467326...  \n",
       "2  [-1.0550424, -0.4334481, -1.2319748, 0.5235287...  \n",
       "3  [2.9089928, -1.321404, -3.329828, -1.644749, 0...  \n",
       "4  [0.34323144, -0.16988352, -0.8524246, -0.33802...  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to csv\n",
    "df.to_csv(\"./data/ABIDE_tab_compressed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
